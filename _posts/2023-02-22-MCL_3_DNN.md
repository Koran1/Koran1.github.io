---
layout: post
title: MCL_Day 4,5
subtitle: DNN model, CNN model
gh-repo: daattali/beautiful-jekyll
gh-badge: 
tags: [MCL_Internship]
comments: true
---

 MCL_Internship 4일차와 5일차 내용들을 간략하게 정리한 글들이다. (사진의 출처들은 모두 [MCL](https://mcl.korea.ac.kr/) 에서 제공한 학습 자료들이다. 학습의 목적으로 코드에 주석을 달면서 해석하였지만 문제가 발생한다면 바로 삭제할 것이다.)
 
## MNIST dataset

 MNIST(Modified National Institute of Standards and Technology) dataset 은 손으로 0 ~ 9 까지 숫자들을 쓴 것을 28 X 28 픽셀 크기로 저장된 데이터베이스이다. 글씨는 모두 흑백 처리되어 한 개의 채널 : gray channel을 사용하게 된다. 해당 데이터베이스에는 6만 개의 트레이닝용 이미지와 만 개의 테스트용 이미지들이 있다. 
 
  <img src="/assets/img/MCL/220px-MnistExamples.png" width="50%" height="50%"> 
  
 [MNIST 데이터셋의 샘플 이미지](https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4)
 
  해당 dataset을 활용해 ML에 많이 사용되는데 숫자 이미지를 입력 받으면 어떤 숫자인지 판별하는 (Classification) 역할을 수행하게 된다. 데이터 셋은 다음 [사이트](http://yann.lecun.com/exdb/mnist/) 에서 4 개의 압축 파일을 다운받을 수 있다. (Train image, Train label, Test image, Test label) 
  
~~~
class MNIST_Train(data.Dataset):
    def __init__(self):
        self.bytes_images = list(open(f"dataset/train-images.idx3-ubyte", "rb").read()[16:])
        self.bytes_labels = list(open("dataset/train-labels.idx1-ubyte", "rb").read()[8:])

    def __getitem__(self, idx):
        image = torch.Tensor(self.bytes_images[idx * 784:(idx + 1) * 784])  
        # idx 를 받았을 때 이미지는 784개 일자로 저장되어 있음 (28 X 28 - 784)
        image = torch.reshape(image, (1, 28, 28)) / 255.0                  
        # 0 ~ 1 사이 값으로 normailze
        label = self.bytes_labels[idx]
        # Ground Truth
        return image, label

    def __len__(self):
        return len(self.bytes_labels)
~~~
 
 Test용도 동일하게 정의가 가능하다. 잘 불러왔는지 확인하기 위해 임의로 불러오기 위해서는 다음을 실행하면 된다.
 
~~~
train_dataset = MNIST_Train()
i = random.randrange(len(train_dataset))
image_train, label_train = train_dataset[i]
image_train_PIL = transforms.ToPILImage()(image_train)
display(image_train_PIL)
print(label_train)
print()
~~~

  <img src="/assets/img/MCL/MNIST_checktrain.png" width="50%" height="50%"> 
  
  위 그림과 같은 결과가 나오게 된다.

## DNN (Deep Neural Network) model
 Hidden layer의 개수가 2개 이상인 Neural Network들을 통칭한다. MNIST의 경우에는 Input Layer에는 이미지 픽셀들이 1X28X28 총 784개가 들어가면 Output Layer에는 Classification을 한 10개의 (0 ~ 9) 결과가 나오게 된다. 학습 시에는 dataset에서 mini-batch 단위로 불러오는데 이를 모아 하나의 입력 Tensor가 구성된다. 예로 Batch size = 6 인 경우, 6 X 1 X 28 X 28이 구성된다. 추가로 네트워크 내부의 수많은 layer들을 지날 때마다 입력의 분포가 바뀌는 문제(Internal Covaraince Shift)가 발생한다. 즉, 서로 다른 변수들 사이에 의존도가 있어 layer들을 지날 때마다 결과에 치명적인 불안정한 학습이 유발된다. 이를 해결하기 위해 Batch Normalization을 실행해주면 된다. 
 
 직접 model을 구현한 코드를 살펴보면 다음과 같다.
 
~~~
class Linear_Model_v0(nn.Module):
    def __init__(self, is_BN=False):
        super(Linear_Model_v0, self).__init__()
        self.flatten = nn.Flatten()
        # 28*28 을 일렬로 폄

        self.linear = nn.Sequential(
            nn.Linear(784, 10),
            nn.Softmax(dim=1)
        )
        # 바로 크기 10의 확률분포 (마치 1차 Regression)

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear(x)
        return logits
        # nn.Module에 공통적으로 들어가 있는 함수


class Linear_Model_v1(Linear_Model_v0):
    def __init__(self, is_BN=True):
        super().__init__(is_BN) 
        # is_BN의 역할은 batch normalization 효과 보기

        if is_BN:   # 첫 선언 일때
            self.linear = nn.Sequential(
                nn.Linear(784, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Linear(128, 64),
                # Hidden Layer 1
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Linear(64, 10),
                nn.Softmax(dim=1)
            )
        else:
            self.linear = nn.Sequential(
                nn.Linear(784, 128),
                nn.ReLU(),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 10),
                nn.Softmax(dim=1)
            )
# v0 과 v1 은 layer 개수가 다름
~~~

Train하는 model은 위의 선언한 model들을 사용하여 

~~~
def train_model(info):
    if info['is_BN']:
        isBN = f'withBN'
    else:
        isBN = f'withoutBN'

    print(f"Model : {info['model']}, Batch Normalization : {isBN}")
    epochs = info["epochs"]

    writer = info["writer"]
    # 중간 중간 학습이 제대로 되는지 확인하는 역할
    ckpt_path = os.path.join('checkpoints', f"{info['model']}_{isBN}")
    if not os.path.exists('checkpoints'): os.mkdir('checkpoints')
    if not os.path.exists(ckpt_path): os.mkdir(ckpt_path)

    train_dataset = mnist_train()
    test_dataset = mnist_test()
    train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=info["batch_size"], shuffle=True,
                                       pin_memory=True)
    test_dataloader = data.DataLoader(dataset=test_dataset, batch_size=info["batch_size"], shuffle=False,
                                      pin_memory=True)

    # 모델 선언, gpu loading
    if info["model"] == "Linear_Model_v0":
        model = Linear_Model_v0(info["is_BN"])
    elif info["model"] == "Linear_Model_v1":
        model = Linear_Model_v1(info["is_BN"])
    else:
        pass
    model.cuda()

    # Loss Function, Optimizer 선언
    # 출력이 pmf 지만 최근 출력은 영상으로 나오므로 PSNR 같은 함수 사용
    # Loss ft. : MSE
    if info["loss"] == "MSE":
        mse = nn.MSELoss().cuda()
        def MSELoss(logit, label):
            return mse(logit, torch.nn.functional.one_hot(label, num_classes=10).float())
        loss_function = MSELoss
    # Loss ft. : CE
    elif info["loss"] == "CE":
        loss_function = nn.CrossEntropyLoss().cuda()
    else:
        pass
    # Gradient-Descent optimizer을 
    optimizer = torch.optim.SGD(model.parameters(), lr=info["lr"])

    # epoch별 loss, test acc 기록할 list
    log_loss_train = []
    log_loss_test = []
    log_acc_test = []

    # epoch 반복문, iteration, 네트워크 train, epoch별 test
    for epoch in range(epochs):
        # train 준비
        start_time = time()
        train_loss_per_epoch = 0
        # model parameter 가 변하도록 냅두는 mode
        model.train()

        # mini batch 불러오기 반복문
        for iter, batch in enumerate(train_dataloader):
            image = batch[0].cuda()
            label = batch[1].cuda()

            y = model(image) # y size : Batch x 10

            loss_train = loss_function(y, label)

            optimizer.zero_grad()
            loss_train.backward()
            optimizer.step()

            # 학습 결과 기록 -> 확률로
            train_loss_per_epoch += loss_train.item() / len(train_dataloader)

            # tensorboard에 step별 train loss 저장
            writer.add_scalar(f"{info['model']}_{isBN}/Train_Loss", loss_train, epoch * len(train_dataloader) + iter)

        # 에폭별 train 결과 출력해보기 -> 얘를 잘 지켜보면서 학습 경과를 확인
        print(f"Epoch[{epoch}] Train Loss: {train_loss_per_epoch:.4f}", end="")
        log_loss_train.append(train_loss_per_epoch)
 
        # test 준비 -> model parameter 가 변하지 않는 mode
        model.eval()
        loss_test_epoch = 0
        total_num_accs = 0

        # test 시에 tensor에 gradient 위한 기록 남길 필요 없음
        with torch.no_grad():
            # test에서 mini-batch 불러오기
            for iter, batch in enumerate(test_dataloader):
                image = batch[0].cuda()
                label = batch[1].cuda()

                y = model(image) # size of y : Batch x 10
                loss_test = loss_function(y, label)
                
                loss_test_epoch += loss_test / len(test_dataloader)

                y_ = torch.argmax(y, dim=1) # size of y_ : batch x 1
                # accuracy 
                num_accs = torch.sum(y_ == label).item()
                total_num_accs += num_accs / (len(test_dataloader) * info["batch_size"])

        # tensorboard에 test 결과 기록 (Loss, Accuracy)
        writer.add_scalar(f"{info['model']}_{isBN}/Test_Loss", loss_test_epoch, epoch)
        writer.add_scalar(f"{info['model']}_{isBN}/Test_Accuracy", total_num_accs * 100, epoch)
        log_loss_test.append(loss_test_epoch.item())
        log_acc_test.append(total_num_accs * 100)

        time_per_epoch = time() - start_time
        print( f" Test Loss: {loss_test_epoch:.4f} Accuracy Rate: {total_num_accs * 100:.2f}% Time:     {time_per_epoch:.2f}sec")

    return log_loss_train, log_loss_test, log_acc_test
~~~

이후 학습을 시키면서 각 epoch 마다 train loss, test loss, accuracy rate, time을 기록하는 코드를 실행해 보자. 성능에 따라 다르겠지만 약 25분(25:03) 정도 걸렸으며 epoch마다 6~10초 사이 걸렸다.

~~~
if not os.path.exists('logs'): os.mkdir('logs')
writer = SummaryWriter('logs')

# epoch, learning rate, model 등을 선언
info = {
    "epochs" : 50,
    "batch_size" : 500,
    "lr" : 0.1,
    "model" : "Linear_Model_v0",
    "is_BN" : False,
    "writer" : writer,
    "loss": "MSE",
}
# 초반에 학습 속도가 빠르고 후반으로 갈수록 느려짐
linear_v0_withoutBN = train_model(info)
print()

info["model"] = "Linear_Model_v1"
info["is_BN"] = False
linear_v1_withoutBN = train_model(info)
print()
# batch Normalization이 중요하다! -> batch size 가 클수록 영향이 많음
info["is_BN"] = True
linear_v1_withBN = train_model(info)
print()
# Time아 실성능에서 굉장히 중요하다!
# Loss ft.을 MSE가 아닌 Cross Entropy를 사용하였을 때 학습을 비교하는 용도
info["loss"] = "CE"
linear_v1_withBN_CE = train_model(info)
print()
~~~

  <img src="/assets/img/MCL/MNIST_trainex.png" >

이후 matplotlib를 활용하여 시각화 가능하다. withBN은 Batch Normalizaion을 한 경우를 뜻하며 Linear_v0는 그냥 Linear model, Linear_v1은 DNN 형태를 가지고 있다. 

### Train Loss

  <img src="/assets/img/MCL/MNIST_training_loss_Linear.png" >
  
### Test Loss 

  <img src="/assets/img/MCL/MNIST_test_loss_Linear.png" >

 Train과 Test Loss (MSE만 비교, CE의 경우는 비교하기 어려움) 두 경우 모두 매우 비슷한 결과를 보인다. 추가로 Linear_v1는 withoutBN 즉, Batch Normalization이 없는 경우 Linear_v0보다 더 큰 Loss를 초반 epoch에서 보여주지만 둘 다 41 epoch 때 Loss : 0.201 로 동일하다가 후반에는 Linear_v1_withoutBN이 더 낮은 loss를 보여주는 것을 확인할 수 있다. 하지만 Linear_v1_withBN이 두 경우를 모든 epoch에서 비해 낮은 loss를 보여줘 더 좋은 성능을 가진 network임을 알 수 있다.
 
### Test Accuracy

<img src="/assets/img/MCL/MNIST_test_acc_Linear.png" >
  
 Test Accuracy의 경우에는 Loss ft.과 관계없이 비교가 가능한데 CE를 사용한 경우가 MSE를 사용한 경우보다 더 좋은 정확도를 가지는 것을 확인할 수 있다. Linear_v0_withoutBN 보다는 Linear_v1_withBN이 더 좋은 성능을 내는 것을 통해 hidden layer를 통해 더 좋은 성능을 보여주는 것을 확인할 수 있지만 Linear_v1_withoutBN과 비교하면 Batch Normalization이 굉장히 중요한 역할을 한다는 것 역시 확인할 수 있다. 추가로 대부분 epoch 초반에는 성능이 크게 향상하다가 어느 정도 이후에는 거의 일정한 정확도에 수렴하는 형식의 그래프들을 확인할 수 있다. 그래서 학습 시 중간중간 확인하면서 적당한 epoch을 설정하고 Loss ft.을 선택하는 것이 실성능과 바로 이어진다.
  
## CIFAR-10 dataset
 CIFAR(Canadian Institute For Advanced Research) - 10 dataset 은 10개의 class로 분류되는 물체들을 담은 이미지 데이터베이스이다. 5만 장의 Training용, 만 장의 Test용 총 6만 장의 이미지들이 존재하며 3(RGB) X 32 X 32 이미지 픽셀들로 이루어져 있다. 각 클래스들은 비행기(airplane), 차(automobile), 새(bird), 고양이(cat), 사슴(deer), 개(dog), 개구리(frog), 말(horse), 배(ship), 트럭(truck)로 구성되어 있으며 CIFAR-100 dataset도 존재한다. 
  
  
