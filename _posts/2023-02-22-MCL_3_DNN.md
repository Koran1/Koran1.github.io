---
layout: post
title: MCL_Day 4,5
subtitle: DNN model, CNN model
gh-repo: daattali/beautiful-jekyll
gh-badge: 
tags: [MCL_Internship]
comments: true
---

 MCL_Internship 4일차와 5일차 내용들을 간략하게 정리한 글들이다. (사진의 출처들은 모두 [MCL](https://mcl.korea.ac.kr/) 에서 제공한 학습 자료들이다. 학습의 목적으로 코드에 주석을 달면서 해석하였지만 문제가 발생한다면 바로 삭제할 것이다.)
 
## MNIST dataset

 MNIST(Modified National Institute of Standards and Technology) dataset 은 손으로 0 ~ 9 까지 숫자들을 쓴 것을 28 X 28 픽셀 크기로 저장된 데이터베이스이다. 글씨는 모두 흑백 처리되어 한 개의 채널 : gray channel을 사용하게 된다. 해당 데이터베이스에는 6만 개의 트레이닝용 이미지와 만 개의 테스트용 이미지들이 있다. 

  ![220px-MnistExamples](https://user-images.githubusercontent.com/32359257/220646393-55bb573d-54a2-455d-85f5-d9b4b5942f6d.png)

 [MNIST 데이터셋의 샘플 이미지](https://ko.wikipedia.org/wiki/MNIST_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4)
 
  해당 dataset을 활용해 ML에 많이 사용되는데 숫자 이미지를 입력 받으면 어떤 숫자인지 판별하는 (Classification) 역할을 수행하게 된다. 데이터 셋은 다음 [사이트](http://yann.lecun.com/exdb/mnist/) 에서 4 개의 압축 파일을 다운받을 수 있다. (Train image, Train label, Test image, Test label) 
  
~~~
class MNIST_Train(data.Dataset):
    def __init__(self):
        self.bytes_images = list(open(f"dataset/train-images.idx3-ubyte", "rb").read()[16:])
        self.bytes_labels = list(open("dataset/train-labels.idx1-ubyte", "rb").read()[8:])

    def __getitem__(self, idx):
        image = torch.Tensor(self.bytes_images[idx * 784:(idx + 1) * 784])  
        # idx 를 받았을 때 이미지는 784개 일자로 저장되어 있음 (28 X 28 - 784)
        image = torch.reshape(image, (1, 28, 28)) / 255.0                  
        # 0 ~ 1 사이 값으로 normailze
        label = self.bytes_labels[idx]
        # Ground Truth
        return image, label

    def __len__(self):
        return len(self.bytes_labels)
~~~
 
 Test용도 동일하게 정의가 가능하다. 잘 불러왔는지 확인하기 위해 임의로 불러오기 위해서는 다음을 실행하면 된다.
 
~~~
train_dataset = MNIST_Train()
i = random.randrange(len(train_dataset))
image_train, label_train = train_dataset[i]
image_train_PIL = transforms.ToPILImage()(image_train)
display(image_train_PIL)
print(label_train)
print()
~~~

  ![MNIST_checktrain](https://user-images.githubusercontent.com/32359257/220646502-202ad2f5-05f3-45a0-8493-2de51f7729a0.PNG)

  위 그림과 같은 결과가 나오게 된다.

## DNN (Deep Neural Network) model
 Hidden layer의 개수가 2개 이상인 Neural Network들을 통칭한다. MNIST의 경우에는 Input Layer에는 이미지 픽셀들이 1X28X28 총 784개가 들어가면 Output Layer에는 Classification을 한 10개의 (0 ~ 9) 결과가 나오게 된다. 학습 시에는 dataset에서 mini-batch 단위로 불러오는데 이를 모아 하나의 입력 Tensor가 구성된다. 예로 Batch size = 6 인 경우, 6 X 1 X 28 X 28이 구성된다. 추가로 네트워크 내부의 수많은 layer들을 지날 때마다 입력의 분포가 바뀌는 문제(Internal Covaraince Shift)가 발생한다. 즉, 서로 다른 변수들 사이에 의존도가 있어 layer들을 지날 때마다 결과에 치명적인 불안정한 학습이 유발된다. 이를 해결하기 위해 Batch Normalization을 실행해주면 된다. 
 
 직접 model을 구현한 코드를 살펴보면 다음과 같다.
 
~~~
class Linear_Model_v0(nn.Module):
    def __init__(self, is_BN=False):
        super(Linear_Model_v0, self).__init__()
        self.flatten = nn.Flatten()
        # 28*28 을 일렬로 폄

        self.linear = nn.Sequential(
            nn.Linear(784, 10),
            nn.Softmax(dim=1)
        )
        # 바로 크기 10의 확률분포 (마치 1차 Regression)

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear(x)
        return logits
        # nn.Module에 공통적으로 들어가 있는 함수


class Linear_Model_v1(Linear_Model_v0):
    def __init__(self, is_BN=True):
        super().__init__(is_BN) 
        # is_BN의 역할은 batch normalization 효과 보기

        if is_BN:   # 첫 선언 일때
            self.linear = nn.Sequential(
                nn.Linear(784, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Linear(128, 64),
                # Hidden Layer 1
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Linear(64, 10),
                nn.Softmax(dim=1)
            )
        else:
            self.linear = nn.Sequential(
                nn.Linear(784, 128),
                nn.ReLU(),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 10),
                nn.Softmax(dim=1)
            )
# v0 과 v1 은 layer 개수가 다름
~~~

Train하는 model은 위의 선언한 model들을 사용하여 

~~~
def train_model(info):
    if info['is_BN']:
        isBN = f'withBN'
    else:
        isBN = f'withoutBN'

    print(f"Model : {info['model']}, Batch Normalization : {isBN}")
    epochs = info["epochs"]

    writer = info["writer"]
    # 중간 중간 학습이 제대로 되는지 확인하는 역할
    ckpt_path = os.path.join('checkpoints', f"{info['model']}_{isBN}")
    if not os.path.exists('checkpoints'): os.mkdir('checkpoints')
    if not os.path.exists(ckpt_path): os.mkdir(ckpt_path)

    train_dataset = mnist_train()
    test_dataset = mnist_test()
    train_dataloader = data.DataLoader(dataset=train_dataset, batch_size=info["batch_size"], shuffle=True,
                                       pin_memory=True)
    test_dataloader = data.DataLoader(dataset=test_dataset, batch_size=info["batch_size"], shuffle=False,
                                      pin_memory=True)

    # 모델 선언, gpu loading
    if info["model"] == "Linear_Model_v0":
        model = Linear_Model_v0(info["is_BN"])
    elif info["model"] == "Linear_Model_v1":
        model = Linear_Model_v1(info["is_BN"])
    else:
        pass
    model.cuda()

    # Loss Function, Optimizer 선언
    # 출력이 pmf 지만 최근 출력은 영상으로 나오므로 PSNR 같은 함수 사용
    # Loss ft. : MSE
    if info["loss"] == "MSE":
        mse = nn.MSELoss().cuda()
        def MSELoss(logit, label):
            return mse(logit, torch.nn.functional.one_hot(label, num_classes=10).float())
        loss_function = MSELoss
    # Loss ft. : CE
    elif info["loss"] == "CE":
        loss_function = nn.CrossEntropyLoss().cuda()
    else:
        pass
    # Gradient-Descent optimizer을 
    optimizer = torch.optim.SGD(model.parameters(), lr=info["lr"])

    # epoch별 loss, test acc 기록할 list
    log_loss_train = []
    log_loss_test = []
    log_acc_test = []

    # epoch 반복문, iteration, 네트워크 train, epoch별 test
    for epoch in range(epochs):
        # train 준비
        start_time = time()
        train_loss_per_epoch = 0
        # model parameter 가 변하도록 냅두는 mode
        model.train()

        # mini batch 불러오기 반복문
        for iter, batch in enumerate(train_dataloader):
            image = batch[0].cuda()
            label = batch[1].cuda()

            y = model(image) # y size : Batch x 10

            loss_train = loss_function(y, label)

            optimizer.zero_grad()
            loss_train.backward()
            optimizer.step()

            # 학습 결과 기록 -> 확률로
            train_loss_per_epoch += loss_train.item() / len(train_dataloader)

            # tensorboard에 step별 train loss 저장
            writer.add_scalar(f"{info['model']}_{isBN}/Train_Loss", loss_train, epoch * len(train_dataloader) + iter)

        # 에폭별 train 결과 출력해보기 -> 얘를 잘 지켜보면서 학습 경과를 확인
        print(f"Epoch[{epoch}] Train Loss: {train_loss_per_epoch:.4f}", end="")
        log_loss_train.append(train_loss_per_epoch)
 
        # test 준비 -> model parameter 가 변하지 않는 mode
        model.eval()
        loss_test_epoch = 0
        total_num_accs = 0

        # test 시에 tensor에 gradient 위한 기록 남길 필요 없음
        with torch.no_grad():
            # test에서 mini-batch 불러오기
            for iter, batch in enumerate(test_dataloader):
                image = batch[0].cuda()
                label = batch[1].cuda()

                y = model(image) # size of y : Batch x 10
                loss_test = loss_function(y, label)
                
                loss_test_epoch += loss_test / len(test_dataloader)

                y_ = torch.argmax(y, dim=1) # size of y_ : batch x 1
                # accuracy 
                num_accs = torch.sum(y_ == label).item()
                total_num_accs += num_accs / (len(test_dataloader) * info["batch_size"])

        # tensorboard에 test 결과 기록 (Loss, Accuracy)
        writer.add_scalar(f"{info['model']}_{isBN}/Test_Loss", loss_test_epoch, epoch)
        writer.add_scalar(f"{info['model']}_{isBN}/Test_Accuracy", total_num_accs * 100, epoch)
        log_loss_test.append(loss_test_epoch.item())
        log_acc_test.append(total_num_accs * 100)

        time_per_epoch = time() - start_time
        print( f" Test Loss: {loss_test_epoch:.4f} Accuracy Rate: {total_num_accs * 100:.2f}% Time:     {time_per_epoch:.2f}sec")

    return log_loss_train, log_loss_test, log_acc_test
~~~

이후 학습을 시키면서 각 epoch 마다 train loss, test loss, accuracy rate, time을 기록하는 코드를 실행해 보자. 성능에 따라 다르겠지만 약 25분(25:03) 정도 걸렸으며 epoch마다 6~10초 사이 걸렸다.

~~~
if not os.path.exists('logs'): os.mkdir('logs')
writer = SummaryWriter('logs')

# epoch, learning rate, model 등을 선언
info = {
    "epochs" : 50,
    "batch_size" : 500,
    "lr" : 0.1,
    "model" : "Linear_Model_v0",
    "is_BN" : False,
    "writer" : writer,
    "loss": "MSE",
}
# 초반에 학습 속도가 빠르고 후반으로 갈수록 느려짐
linear_v0_withoutBN = train_model(info)
print()

info["model"] = "Linear_Model_v1"
info["is_BN"] = False
linear_v1_withoutBN = train_model(info)
print()
# batch Normalization이 중요하다! -> batch size 가 클수록 영향이 많음
info["is_BN"] = True
linear_v1_withBN = train_model(info)
print()
# Time아 실성능에서 굉장히 중요하다!
# Loss ft.을 MSE가 아닌 Cross Entropy를 사용하였을 때 학습을 비교하는 용도
info["loss"] = "CE"
linear_v1_withBN_CE = train_model(info)
print()
~~~

![MNIST_trainex](https://user-images.githubusercontent.com/32359257/220646827-30a7eda5-14b9-4fd5-9b11-24d16ea6ce9d.PNG)


이후 matplotlib를 활용하여 시각화 가능하다. withBN은 Batch Normalizaion을 한 경우를 뜻하며 Linear_v0는 그냥 Linear model, Linear_v1은 DNN 형태를 가지고 있다. 

### Train Loss

![MNIST_training_loss_Linear](https://user-images.githubusercontent.com/32359257/220646581-360e7dfb-bff3-4c6a-981b-b296695a88e6.png)
  
### Test Loss 

![MNIST_test_loss_Linear](https://user-images.githubusercontent.com/32359257/220646623-b14765e8-60f4-4e2f-896d-1b5612b935ee.png)

 Train과 Test Loss (MSE만 비교, CE의 경우는 비교하기 어려움) 두 경우 모두 매우 비슷한 결과를 보인다. 추가로 Linear_v1는 withoutBN 즉, Batch Normalization이 없는 경우 Linear_v0보다 더 큰 Loss를 초반 epoch에서 보여주지만 둘 다 41 epoch 때 Loss : 0.201 로 동일하다가 후반에는 Linear_v1_withoutBN이 더 낮은 loss를 보여주는 것을 확인할 수 있다. 하지만 Linear_v1_withBN이 두 경우를 모든 epoch에서 비해 낮은 loss를 보여줘 더 좋은 성능을 가진 network임을 알 수 있다.
 
### Test Accuracy

![MNIST_test_acc_Linear](https://user-images.githubusercontent.com/32359257/220646664-37d31a13-d0a2-4e25-8268-937006d17d99.png)
  
 Test Accuracy의 경우에는 Loss ft.과 관계없이 비교가 가능한데 CE를 사용한 경우가 MSE를 사용한 경우보다 더 좋은 정확도를 가지는 것을 확인할 수 있다. Linear_v0_withoutBN 보다는 Linear_v1_withBN이 더 좋은 성능을 내는 것을 통해 hidden layer를 통해 더 좋은 성능을 보여주는 것을 확인할 수 있지만 Linear_v1_withoutBN과 비교하면 Batch Normalization이 굉장히 중요한 역할을 한다는 것 역시 확인할 수 있다. 추가로 대부분 epoch 초반에는 성능이 크게 향상하다가 어느 정도 이후에는 거의 일정한 정확도에 수렴하는 형식의 그래프들을 확인할 수 있다. 그래서 학습 시 중간중간 확인하면서 적당한 epoch을 설정하고 Loss ft.을 선택하는 것이 실성능과 바로 이어진다.
  
## CIFAR-10 dataset
 CIFAR(Canadian Institute For Advanced Research) - 10 dataset 은 10개의 class로 분류되는 물체들을 담은 이미지 데이터베이스이다. 5만 장의 Training용, 만 장의 Test용 총 6만 장의 이미지들이 존재하며 3(RGB) X 32 X 32 이미지 픽셀들로 이루어져 있다. 각 클래스들은 비행기(airplane), 차(automobile), 새(bird), 고양이(cat), 사슴(deer), 개(dog), 개구리(frog), 말(horse), 배(ship), 트럭(truck)로 구성되어 있으며 CIFAR-100 dataset도 존재한다.  
 해당 dataset을 torchvision 라이브러리에서 다운로드 가능하며 MNIST도 가능하다.
  
~~~
# data를 Tensor로 변환
transform = transforms.Compose(
    [transforms.ToTensor()]
)

class CIFAR10_sampling(torch.utils.data.Dataset):                    # 총 5만장 dataset 
    def __init__(self, dataset, rate):              
        self.img_list = []
        self.label_list = []
        cnt_list = [0] * 10
        for img, label in dataset:
            if cnt_list[label] < int(len(trainset) // 10 * rate):    # rate = 0.5 면 2만5천장 뽑음
                self.img_list.append(img)
                self.label_list.append(label)
                cnt_list[label] += 1

    def __getitem__(self, idx):
        return self.img_list[idx], self.label_list[idx]

    def __len__(self):
        return len(self.label_list)
        
# 다운 후에는 download = False로 바꿔야함
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                    download=True, transform=transform)
# 10개의 class들
classes = ('plane', 'car', 'bird', 'cat',
        'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
~~~

다음 사진은 말의 사진을 예시로 가져왔다. 3 X 32 X 32라서 화질이 많이 깨진다.

![CIFAR10_horse](https://user-images.githubusercontent.com/32359257/220646699-e99f707c-39bf-4f9a-8aec-1fe7f5cd41a1.PNG)


## CNN(Convolutional Neural Network) model
 앞서 DNN과 달리 Convolution이 들어간 CNN은 전처리 작업이 추가된 모델이다. DNN은 MNIST와 같은 gray channel 즉, channel = 1 인 경우는 잘 처리하지만 channel이 많아지면 flatten 시키는 과정에서 손실이 발생한다. 따라서 convolution을 활용해 한 픽셀과 주변 픽셀들 간의 관계를 필터를 통해 하나의 픽셀에 정보를 담을 수 있게 된다. 하지만 이 경우 경계에 있는 정보들의 경우 손실이 발생할 수 있으므로 zero padding을 활용해 모든 테두리에 0인 픽셀을 추가한다. (stride는 그 필터를 움직이는 단위이다) 
 
 ![CIFAR10](https://user-images.githubusercontent.com/32359257/220653187-8af01364-4a98-4552-b96d-3ea7f5b213fd.png)

위의 사진을 봤을 때 Input을 Convolution + ReLU를 먼저 한다. 이는 특정 크기의 filter(Kernel)를 가지고 convolution을 수행한 후 ReLU를 이용해 활성화 함수를 적용한다. 보통 Tensor의 채널이 변하는 Conv2d Layer이다. 이후 Pooling을 통해 Tensor의 Height와 Width 즉, 해상도를 변경한다. 크게 2 가지 종류가 있는데 1) Max Pooling은 4개의 픽셀 중 가장 큰 값을 가진 픽셀을 대표로 선택하는 것이고, 2) Average Pooling은 4개의 픽셀들을 평균 연산하여 대표 픽셀로 나타낸다. 이로써 크기가 절반으로 줄게 된다. 기본적인 CNN 모델 중 하나인 VGGNet-16의 예시를 살펴보면 다음과 같다.

![CNN_VGG16](https://user-images.githubusercontent.com/32359257/220654646-ba2e4eef-060f-4914-bb0c-43ac49d53515.png)

3 X 224 X 224 이미지를 1 X 1 X 1000 으로 변환하여 이를 이용해 네트워크는 학습을 진행한다.  
Train 모델 코드는 다음과 같았다.
~~~
def train_model(info):
    # model 정의
    if info["model"] == "vgg":
        model = vgg_model()
    elif info["model"] == "linear":
        model = Linear_Model()
    else:
        print("Model Error")
        exit(0)
    model.cuda()

    # checkpoints 저장 디렉토리 만들기, tensorboard 정의
    ckpt_path = info["model"]
    if not os.path.exists(ckpt_path): os.mkdir(ckpt_path)
    pth_path = os.path.join(ckpt_path, "pth")
    if not os.path.exists(pth_path): os.mkdir(pth_path)
    writer = info["writer"]

    print(f"Length of train dataset: {len(trainset)} / {int(len(trainset) / info['train_sampling_rate'])}")

    train_loader = torch.utils.data.DataLoader(trainset, batch_size=info["batch_size"],
                                            shuffle=True, num_workers=2)
    test_loader = torch.utils.data.DataLoader(testset, batch_size=info["batch_size"],
                                            shuffle=False, num_workers=2)

    # Loss 정의, Optimizer, Scheduler
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=info["lr"])
    # 10 epoch 마다 lr를 조정 -> 초반에는 성능 상향 폭이 크기만 후반에는 정밀하게 optimize 해야하므로 lr 감소
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

    # train, test
    log_loss_train = []
    log_loss_test = []
    log_acc_test = []

    for epoch in range(info["epochs"]):
        # train
        start_time = time()
        train_loss_per_epoch = 0
        model.train()

        for iter, batch in enumerate(train_loader):
            image = batch[0].cuda()
            label = batch[1].cuda()

            y = model(image) # size of y : Batch x 10
            loss_train = criterion(y, label)

            optimizer.zero_grad()
            loss_train.backward()
            optimizer.step()

            train_loss_per_epoch += loss_train.item() / len(train_loader)

            writer.add_scalar(f"{info['model']}/Train_Loss", loss_train, epoch * len(train_loader) + iter)
            # if (iter + 1) % 10 == 0:
            #     print(f"\t[{iter + 1}/{len(train_loader)}] Train Loss: {loss_train.item():.4f}")

        print(f"Epoch[{epoch:3d}] Train Loss: {train_loss_per_epoch:.4f}", end='')
        log_loss_train.append(train_loss_per_epoch)

        # test
        model.eval()
        loss_test_epoch = 0
        total_num_accs = 0

        with torch.no_grad():
            for iter, batch in enumerate(test_loader):
                image = batch[0].cuda()
                label = batch[1].cuda()

                y = model(image)
                loss_test = criterion(y, label)
                loss_test_epoch += loss_test / len(test_loader)

                y_ = torch.argmax(y, dim=1)
                num_accs = torch.sum(y_ == label).item()
                total_num_accs += num_accs

        acc_rate = (total_num_accs / len(testset)) * 100
        writer.add_scalar(f"{info['model']}/Test_Loss", loss_test_epoch, epoch)
        writer.add_scalar(f"{info['model']}/Test_Accuracy", acc_rate, epoch)
        log_loss_test.append(loss_test_epoch.item())
        log_acc_test.append(acc_rate)

        time_per_epoch = time() - start_time
        print(f" Test Loss: {loss_test_epoch:.4f}"
              f" Accuracy Rate: {acc_rate:.2f}%"
              f" lr: {scheduler.get_last_lr()[0]:.1E}"
              f" time per epoch: {time_per_epoch:.2f}sec"
              )

        scheduler.step()

    return log_loss_train, log_loss_test, log_acc_test
~~~
 이 때, 코드에서 보면 learning rate를 조절하는 부분이 있는데 초반에는 성능 향상 폭이 크지만 후반 epoch에서는 세밀하게 조정해줘야 하므로 gamma 를 설정해 해당 코드에서는 10 epoch 마다 learning rate를 0.9배 씩 하고 있다. (이 부분은 경험적으로 조정해야 한다고 말해주셨다) epoch 은 15 정도로 설정하여 5만 장에 대한 training을 거친 후 결과는 다음과 같다. (약 5분 (5:35) 정도 걸렸다)
 ### Train Loss
 
 ![train_loss (1)](https://user-images.githubusercontent.com/32359257/220659928-447f8cff-1d3a-4c27-b821-ad53d7d3c2bd.png)

### Test Loss

![test_loss (1)](https://user-images.githubusercontent.com/32359257/220659974-99be30e4-6c70-40c3-87d5-b4d7cc1a9d7d.png)

### Test Accuracy 
 
 ![test_acc (1)](https://user-images.githubusercontent.com/32359257/220660011-35b2a477-d4e5-4447-8615-ce2907477990.png)

(개인적인 견해 : 결과 그래프를 보면 다소 튀는 경우가 많은 걸 보아 확실히 epoch 수가 많아야 안정적인 네트워크 학습이 이루어질 것 같다)
